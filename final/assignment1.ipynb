{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "05ea3e7d-d1ab-4f77-9eb3-30f16df3a353",
    "_uuid": "cc35e2ec-3050-4961-bdb3-67e8e5938042",
    "execution": {
     "iopub.execute_input": "2025-05-18T12:19:39.874133Z",
     "iopub.status.busy": "2025-05-18T12:19:39.873883Z",
     "iopub.status.idle": "2025-05-18T12:19:39.878706Z",
     "shell.execute_reply": "2025-05-18T12:19:39.877925Z",
     "shell.execute_reply.started": "2025-05-18T12:19:39.874117Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "7e120cf5-ecff-4904-8ea6-2b2547ff618e",
    "_uuid": "f21c98de-9940-4858-9aa5-da4db9ec73f7",
    "execution": {
     "iopub.execute_input": "2025-05-18T12:18:41.018649Z",
     "iopub.status.busy": "2025-05-18T12:18:41.017866Z",
     "iopub.status.idle": "2025-05-18T12:18:41.025099Z",
     "shell.execute_reply": "2025-05-18T12:18:41.024366Z",
     "shell.execute_reply.started": "2025-05-18T12:18:41.018619Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class LFWDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        lfw_people = fetch_lfw_people(min_faces_per_person=20, color=True)\n",
    "        \n",
    "        self.images = lfw_people.images\n",
    "        self.labels = lfw_people.target\n",
    "        self.target_names = lfw_people.target_names\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = torch.from_numpy(image)\n",
    "        image = image.permute(2, 0, 1)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "bb9cc0de-0e14-437d-ab18-5239a8037db3",
    "_uuid": "1967d595-625e-4983-a0f5-636004634c48",
    "execution": {
     "iopub.execute_input": "2025-05-18T12:19:42.316514Z",
     "iopub.status.busy": "2025-05-18T12:19:42.316206Z",
     "iopub.status.idle": "2025-05-18T12:19:42.324721Z",
     "shell.execute_reply": "2025-05-18T12:19:42.323877Z",
     "shell.execute_reply.started": "2025-05-18T12:19:42.316492Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1),\n",
    "\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  \n",
    "        )\n",
    "\n",
    "        \n",
    "        self.embedding = nn.Linear(512, embedding_size, bias=False)\n",
    "        self.bn_embedding = nn.BatchNorm1d(embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)           \n",
    "        x = x.view(x.size(0), -1)         \n",
    "        x = self.embedding(x)             \n",
    "        x = self.bn_embedding(x)          \n",
    "        x = F.normalize(x, p=2, dim=1)    \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, embedding_size, num_classes, scale=64.0, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        cosine = F.linear(F.normalize(embeddings), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.clamp(cosine ** 2, 0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = F.one_hot(labels, num_classes=self.weight.size(0)).float().to(embeddings.device)\n",
    "        logits = one_hot * phi + (1.0 - one_hot) * cosine\n",
    "        logits *= self.scale\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dataset = LFWDataset(transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "num_classes = len(np.unique(dataset.labels))\n",
    "\n",
    "model = Model(embedding_size=128).to(device)\n",
    "arcface = ArcFaceLoss(embedding_size=128, num_classes=num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(arcface.parameters()), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "6e0e9f5e-0727-4204-99f2-d3e7e11ff023",
    "_uuid": "b2943ae1-e003-4ee5-b16f-109278e6ee17",
    "execution": {
     "iopub.execute_input": "2025-05-18T12:19:47.329118Z",
     "iopub.status.busy": "2025-05-18T12:19:47.328833Z",
     "iopub.status.idle": "2025-05-18T12:22:19.093156Z",
     "shell.execute_reply": "2025-05-18T12:22:19.092455Z",
     "shell.execute_reply.started": "2025-05-18T12:19:47.329093Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:31<00:00,  3.04s/it, loss=0.00743] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 50\n",
    "loss_history = []\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "\n",
    "for epoch in pbar:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        embeddings = model(images)\n",
    "        logits = arcface(embeddings, labels)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    pbar.set_postfix({'loss': loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "aae7b755-ff7e-4e31-b26e-108b0f3e1c25",
    "_uuid": "bfd49692-a469-4c5b-8767-5bdde2c23eef",
    "execution": {
     "iopub.execute_input": "2025-05-18T12:22:22.320637Z",
     "iopub.status.busy": "2025-05-18T12:22:22.320281Z",
     "iopub.status.idle": "2025-05-18T12:22:22.442942Z",
     "shell.execute_reply": "2025-05-18T12:22:22.442096Z",
     "shell.execute_reply.started": "2025-05-18T12:22:22.320607Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d1e2f612-08bf-4996-afc8-d296d8abaff1",
    "_uuid": "e6de79ee-d21a-4aee-afd9-445a399c8ab8",
    "execution": {
     "iopub.execute_input": "2025-05-16T15:30:20.628736Z",
     "iopub.status.busy": "2025-05-16T15:30:20.628526Z",
     "iopub.status.idle": "2025-05-16T15:30:20.634002Z",
     "shell.execute_reply": "2025-05-16T15:30:20.633312Z",
     "shell.execute_reply.started": "2025-05-16T15:30:20.628719Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def recognize(query_embedding, database, threshold=0.7, device='cpu'):\n",
    "    best_match = None\n",
    "    best_score = -1\n",
    "\n",
    "    query_embedding = query_embedding.squeeze().to(device)\n",
    "\n",
    "    for name, embs in database.items():\n",
    "        total = 0\n",
    "        for emb in embs:\n",
    "            emb = emb.float().squeeze().to(device)\n",
    "\n",
    "            total += F.cosine_similarity(query_embedding, emb, dim=0).item()\n",
    "            \n",
    "        avg = total / len(embs)\n",
    "        if avg > threshold and avg > best_score:\n",
    "            best_score = avg\n",
    "            best_match = name\n",
    "\n",
    "    return best_match or \"Unknown\", best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9a75b320-69d0-4f97-a19b-c61ba8cc66cc",
    "_uuid": "59c5c8cf-de90-4ab1-851c-d964d14fedeb",
    "execution": {
     "iopub.execute_input": "2025-05-16T15:30:20.635146Z",
     "iopub.status.busy": "2025-05-16T15:30:20.634864Z",
     "iopub.status.idle": "2025-05-16T15:30:21.014941Z",
     "shell.execute_reply": "2025-05-16T15:30:21.014134Z",
     "shell.execute_reply.started": "2025-05-16T15:30:20.635123Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def preprocess_face(face_img):\n",
    "    face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB) \n",
    "    face_img = np.transpose(face_img, (2, 0, 1))\n",
    "    face_tensor = transform(torch.tensor(face_img,dtype=torch.float32)).unsqueeze(0)\n",
    "    return face_tensor\n",
    "\n",
    "\n",
    "def recognize_face(face, database, model, threshold=0.7, device='cpu'):\n",
    "    with torch.no_grad():\n",
    "        embedding = model(face)\n",
    "\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "\n",
    "    for name, embs in database.items():\n",
    "        embs_tensor = torch.stack(embs).to(device)\n",
    "        sims = F.cosine_similarity(embedding.unsqueeze(0), embs_tensor, dim=2)\n",
    "        avg = sims.mean().item()\n",
    "        std = sims.std().item()\n",
    "        # print(avg)\n",
    "\n",
    "        if avg - std > threshold and avg - std > best_score:\n",
    "            best_score = avg\n",
    "            best_match = name\n",
    "\n",
    "    return best_match, best_score\n",
    "\n",
    "def process_stream(url, model, database_name = 'data.pkl', threshold=0.7, mode='test', user='user', device='cpu'):\n",
    "    cap = cv2.VideoCapture(url)\n",
    "    database = {user: []}\n",
    "\n",
    "    try:\n",
    "        if mode == 'test':\n",
    "            with open(database_name, 'rb') as f:\n",
    "                database = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print('error opening database')\n",
    "        return\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"cannot open video stream\")\n",
    "        return\n",
    "\n",
    "    recognized = set()\n",
    "\n",
    "    training = False\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale( gray, scaleFactor=1.1, minNeighbors=5, minSize=(128, 128))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            try:\n",
    "                face_tensor = preprocess_face(face_img).to(device)\n",
    "\n",
    "                if mode == 'train':\n",
    "                    color = (0, 0, 255)\n",
    "                    if training:\n",
    "                        color = (0, 165, 220)\n",
    "                        with torch.no_grad():\n",
    "                            embedding = model(face_tensor)\n",
    "                            database[user].append(embedding)\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                    break\n",
    "\n",
    "                name, confidence = recognize_face(face_tensor, database, model, threshold, device=device)\n",
    "\n",
    "                color = (0, 0, 255) if not name else (0, 255, 0)\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                \n",
    "                if name:\n",
    "                    recognized.add(name)\n",
    "                    cv2.rectangle(frame, (x-12, y+h - int((h*confidence))), (x-6, y+h), color, -1)\n",
    "                    cv2.rectangle(frame, (x-13, y+h), (x-5, y), (255, 255, 255))\n",
    "                    cv2.putText(frame, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"error processing face: {e}\")\n",
    "                continue\n",
    "\n",
    "        for i, name in enumerate(recognized):\n",
    "            cv2.putText(frame, name, (30, i * 20 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (220, 220, 0), 2)\n",
    "        \n",
    "        cv2.imshow('Registering Face' if mode == 'train' else 'Recognition Mode', frame)\n",
    "\n",
    "        pressed_key = cv2.waitKey(1) & 0xFF\n",
    "        if pressed_key == ord('s'):\n",
    "            training = not training and mode == 'train'\n",
    "        if pressed_key == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "    if os.path.exists(database_name):\n",
    "        with open(database_name, 'rb') as f:\n",
    "            old = pickle.load(f)\n",
    "            \n",
    "        merged = old.copy()\n",
    "        for k in database:\n",
    "            if k in merged:\n",
    "                merged[k] += database[k]\n",
    "            else:\n",
    "                merged[k] = database[k]\n",
    "\n",
    "    else:\n",
    "        merged = database\n",
    "    \n",
    "    if mode == 'train':\n",
    "        with open(database_name, 'wb') as f:\n",
    "            pickle.dump(merged, f)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5e21f409-fa2c-4069-9a4a-5ae2808c7d62",
    "_uuid": "b6f33a9a-c6ad-4aaf-b9f4-491c4d953370",
    "execution": {
     "iopub.execute_input": "2025-05-16T15:30:21.017043Z",
     "iopub.status.busy": "2025-05-16T15:30:21.016818Z",
     "iopub.status.idle": "2025-05-16T15:30:26.085479Z",
     "shell.execute_reply": "2025-05-16T15:30:26.084344Z",
     "shell.execute_reply.started": "2025-05-16T15:30:21.017026Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "url = 'http://192.168.1.102:4747/video'\n",
    "process_stream(url, model, database_name='data.pkl', threshold=0.7, mode='test', user='john', device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
