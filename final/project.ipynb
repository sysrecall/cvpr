{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765cd40a",
   "metadata": {
    "_cell_guid": "bb2acd7d-0052-4416-82bd-7067ee6344fb",
    "_uuid": "33f21dc7-822a-496d-919f-779e88f69ffe",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-21T18:43:58.653254Z",
     "iopub.status.busy": "2025-06-21T18:43:58.652641Z",
     "iopub.status.idle": "2025-06-21T18:45:29.823108Z",
     "shell.execute_reply": "2025-06-21T18:45:29.822039Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 91.181561,
     "end_time": "2025-06-21T18:45:29.824845",
     "exception": false,
     "start_time": "2025-06-21T18:43:58.643284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=0B0ywwgffWnLLZW9uVHNjb2JmNlE\n",
      "From (redirected): https://drive.google.com/uc?id=0B0ywwgffWnLLZW9uVHNjb2JmNlE&confirm=t&uuid=e9fb23e0-8c9e-4ffc-8be6-432fbf680609\n",
      "To: /kaggle/working/cvpr2016_cub.tar.gz\n",
      "100%|█████████████████████████████████████████| 860M/860M [00:08<00:00, 103MB/s]\n",
      "--2025-06-21 18:44:09--  https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1\n",
      "Resolving data.caltech.edu (data.caltech.edu)... 35.155.11.48\n",
      "Connecting to data.caltech.edu (data.caltech.edu)|35.155.11.48|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://s3.us-west-2.amazonaws.com/caltechdata/96/97/8384-3670-482e-a3dd-97ac171e8a10/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3DCUB_200_2011.tgz&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARCVIVNNAP7NNDVEA%2F20250621%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250621T184409Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=ad8bbdb10ea93815a0bdd28269a14f39aa1cf1e7af66b366210d734df010b5d9 [following]\n",
      "--2025-06-21 18:44:09--  https://s3.us-west-2.amazonaws.com/caltechdata/96/97/8384-3670-482e-a3dd-97ac171e8a10/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3DCUB_200_2011.tgz&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARCVIVNNAP7NNDVEA%2F20250621%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20250621T184409Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=ad8bbdb10ea93815a0bdd28269a14f39aa1cf1e7af66b366210d734df010b5d9\n",
      "Resolving s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)... 52.92.203.128, 52.92.224.248, 52.92.152.56, ...\n",
      "Connecting to s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)|52.92.203.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1150585339 (1.1G) [application/octet-stream]\n",
      "Saving to: ‘CUB_200_2011.tgz?download=1’\n",
      "\n",
      "CUB_200_2011.tgz?do 100%[===================>]   1.07G  14.6MB/s    in 35s     \n",
      "\n",
      "2025-06-21 18:44:44 (31.7 MB/s) - ‘CUB_200_2011.tgz?download=1’ saved [1150585339/1150585339]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download texts\n",
    "!gdown 0B0ywwgffWnLLZW9uVHNjb2JmNlE\n",
    "\n",
    "# download images\n",
    "!wget https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1\n",
    "\n",
    "!mv CUB_200_2011.tgz?download=1 CUB_200_2011.tgz\n",
    "\n",
    "!mkdir raw_texts raw_texts/cvpr2016_cub images\n",
    "\n",
    "!tar -xzf CUB_200_2011.tgz -C images/\n",
    "!tar -xzf cvpr2016_cub.tar.gz -C raw_texts/cvpr2016_cub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "322a374a",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-21T18:45:29.857495Z",
     "iopub.status.busy": "2025-06-21T18:45:29.856912Z",
     "iopub.status.idle": "2025-06-21T18:45:37.691339Z",
     "shell.execute_reply": "2025-06-21T18:45:37.690637Z"
    },
    "papermill": {
     "duration": 7.851966,
     "end_time": "2025-06-21T18:45:37.692885",
     "exception": false,
     "start_time": "2025-06-21T18:45:29.840919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-clip\n",
      "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting ftfy (from openai-clip)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai-clip) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-clip) (4.67.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai-clip) (0.2.13)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-clip\n",
      "  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=d58565d2bdcc3d09d855b87dbb55d0af8ab93e8eea2c06419d2826a7ab1aa573\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/17/90/042948fd2e2a87f1dcf6db6d438cad015c49db0c53d1d9c7dc\n",
      "Successfully built openai-clip\n",
      "Installing collected packages: ftfy, openai-clip\n",
      "Successfully installed ftfy-6.3.1 openai-clip-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-clip"
   ]
  },
  {
   "cell_type": "raw",
   "id": "663b1a76",
   "metadata": {
    "papermill": {
     "duration": 0.015805,
     "end_time": "2025-06-21T18:45:37.725457",
     "exception": false,
     "start_time": "2025-06-21T18:45:37.709652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "!pip install torch-optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229e7ac6",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-21T18:45:37.758816Z",
     "iopub.status.busy": "2025-06-21T18:45:37.758562Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-06-21T18:45:37.741218",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models and optimizers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 157MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11788 image-text pairs.\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600: 100%|██████████| 92/92 [02:24<00:00,  1.57s/it, Lambda_Clip=0.000, Loss_D=-36.0534, Loss_G_Adv=36.4375, Loss_G_Clip=4.8632]\n",
      "Epoch 2/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.010, Loss_D=-10.1469, Loss_G_Adv=-5.6289, Loss_G_Clip=4.8700] \n",
      "Epoch 3/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.020, Loss_D=0.6173, Loss_G_Adv=-5.5312, Loss_G_Clip=4.8917]  \n",
      "Epoch 4/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.030, Loss_D=-4.7137, Loss_G_Adv=4.8125, Loss_G_Clip=4.8865] \n",
      "Epoch 5/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.040, Loss_D=-4.9379, Loss_G_Adv=-4.2344, Loss_G_Clip=4.8739]\n",
      "Epoch 6/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.050, Loss_D=-3.4315, Loss_G_Adv=1.4834, Loss_G_Clip=4.8833] \n",
      "Epoch 7/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.060, Loss_D=-3.9839, Loss_G_Adv=1.0410, Loss_G_Clip=4.8771] \n",
      "Epoch 8/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.070, Loss_D=-5.0884, Loss_G_Adv=-0.3965, Loss_G_Clip=4.8717]\n",
      "Epoch 9/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.080, Loss_D=-5.4706, Loss_G_Adv=0.7310, Loss_G_Clip=4.8743] \n",
      "Epoch 10/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.090, Loss_D=-5.7884, Loss_G_Adv=2.6582, Loss_G_Clip=4.8767] \n",
      "Epoch 11/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.100, Loss_D=-6.3907, Loss_G_Adv=1.1855, Loss_G_Clip=4.8622] \n",
      "Epoch 12/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.110, Loss_D=-3.3890, Loss_G_Adv=4.6094, Loss_G_Clip=4.8678] \n",
      "Epoch 13/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.120, Loss_D=-4.2049, Loss_G_Adv=5.2812, Loss_G_Clip=4.8811] \n",
      "Epoch 14/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.130, Loss_D=-6.1385, Loss_G_Adv=3.7715, Loss_G_Clip=4.8730] \n",
      "Epoch 15/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.140, Loss_D=-5.9175, Loss_G_Adv=0.6118, Loss_G_Clip=4.8752] \n",
      "Epoch 16/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.150, Loss_D=-4.8168, Loss_G_Adv=3.7637, Loss_G_Clip=4.8837] \n",
      "Epoch 17/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.160, Loss_D=-3.4851, Loss_G_Adv=-2.4941, Loss_G_Clip=4.8680]\n",
      "Epoch 18/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.170, Loss_D=-4.7266, Loss_G_Adv=6.6680, Loss_G_Clip=4.8700] \n",
      "Epoch 19/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.180, Loss_D=-3.6292, Loss_G_Adv=2.7676, Loss_G_Clip=4.8595] \n",
      "Epoch 20/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.190, Loss_D=-5.5077, Loss_G_Adv=5.6641, Loss_G_Clip=4.8792] \n",
      "Epoch 21/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.7461, Loss_G_Adv=7.2852, Loss_G_Clip=4.8721] \n",
      "Epoch 22/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.3209, Loss_G_Adv=1.4492, Loss_G_Clip=4.8846] \n",
      "Epoch 23/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.5670, Loss_G_Adv=4.0117, Loss_G_Clip=4.8709] \n",
      "Epoch 24/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.8672, Loss_G_Adv=6.1367, Loss_G_Clip=4.8945] \n",
      "Epoch 25/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.8210, Loss_G_Adv=-3.1523, Loss_G_Clip=4.8796]\n",
      "Epoch 26/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6234, Loss_G_Adv=0.9028, Loss_G_Clip=4.8730] \n",
      "Epoch 27/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.8636, Loss_G_Adv=0.4426, Loss_G_Clip=4.8699] \n",
      "Epoch 28/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.4527, Loss_G_Adv=0.2219, Loss_G_Clip=4.8515] \n",
      "Epoch 29/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.2336, Loss_G_Adv=6.0156, Loss_G_Clip=4.8919] \n",
      "Epoch 30/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.4845, Loss_G_Adv=-0.0443, Loss_G_Clip=4.8779]\n",
      "Epoch 31/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.2880, Loss_G_Adv=0.5034, Loss_G_Clip=4.8797] \n",
      "Epoch 32/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.5897, Loss_G_Adv=3.2832, Loss_G_Clip=4.8765] \n",
      "Epoch 33/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.7263, Loss_G_Adv=4.0195, Loss_G_Clip=4.8775] \n",
      "Epoch 34/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.6640, Loss_G_Adv=5.1602, Loss_G_Clip=4.8645] \n",
      "Epoch 35/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.9092, Loss_G_Adv=7.1367, Loss_G_Clip=4.8696] \n",
      "Epoch 36/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.4873, Loss_G_Adv=11.8281, Loss_G_Clip=4.8700]\n",
      "Epoch 37/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.0693, Loss_G_Adv=7.2773, Loss_G_Clip=4.8646] \n",
      "Epoch 38/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.1146, Loss_G_Adv=3.4180, Loss_G_Clip=4.8848] \n",
      "Epoch 39/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.2885, Loss_G_Adv=4.8789, Loss_G_Clip=4.8748] \n",
      "Epoch 40/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.4355, Loss_G_Adv=5.8984, Loss_G_Clip=4.8754] \n",
      "Epoch 41/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.5970, Loss_G_Adv=5.8008, Loss_G_Clip=4.8645] \n",
      "Epoch 42/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.1842, Loss_G_Adv=7.9141, Loss_G_Clip=4.8857]\n",
      "Epoch 43/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.8482, Loss_G_Adv=8.8203, Loss_G_Clip=4.8860] \n",
      "Epoch 44/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.1795, Loss_G_Adv=0.5151, Loss_G_Clip=4.8722] \n",
      "Epoch 45/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.9690, Loss_G_Adv=6.5312, Loss_G_Clip=4.8674] \n",
      "Epoch 46/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.9967, Loss_G_Adv=5.3281, Loss_G_Clip=4.8718] \n",
      "Epoch 47/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.9775, Loss_G_Adv=1.7559, Loss_G_Clip=4.8713] \n",
      "Epoch 48/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.4430, Loss_G_Adv=7.3555, Loss_G_Clip=4.8775] \n",
      "Epoch 49/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.2785, Loss_G_Adv=2.0566, Loss_G_Clip=4.8662] \n",
      "Epoch 50/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.7769, Loss_G_Adv=6.6562, Loss_G_Clip=4.8560] \n",
      "Epoch 51/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.5263, Loss_G_Adv=4.2812, Loss_G_Clip=4.8552] \n",
      "Epoch 52/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.7788, Loss_G_Adv=6.9570, Loss_G_Clip=4.8727] \n",
      "Epoch 53/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.5057, Loss_G_Adv=8.5391, Loss_G_Clip=4.8557] \n",
      "Epoch 54/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.6175, Loss_G_Adv=6.5898, Loss_G_Clip=4.8805]\n",
      "Epoch 55/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.2887, Loss_G_Adv=5.5625, Loss_G_Clip=4.8669] \n",
      "Epoch 56/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7325, Loss_G_Adv=4.5312, Loss_G_Clip=4.8635] \n",
      "Epoch 57/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.4950, Loss_G_Adv=7.9570, Loss_G_Clip=4.8770] \n",
      "Epoch 58/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.0379, Loss_G_Adv=6.4609, Loss_G_Clip=4.8667]\n",
      "Epoch 59/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.8668, Loss_G_Adv=6.9844, Loss_G_Clip=4.8672] \n",
      "Epoch 60/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.6736, Loss_G_Adv=2.0996, Loss_G_Clip=4.8630] \n",
      "Epoch 61/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.6618, Loss_G_Adv=7.0820, Loss_G_Clip=4.8684] \n",
      "Epoch 62/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.9472, Loss_G_Adv=5.8359, Loss_G_Clip=4.8488] \n",
      "Epoch 63/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.9102, Loss_G_Adv=5.3164, Loss_G_Clip=4.8614] \n",
      "Epoch 64/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.7172, Loss_G_Adv=7.5859, Loss_G_Clip=4.8604] \n",
      "Epoch 65/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.5704, Loss_G_Adv=0.8062, Loss_G_Clip=4.8511]\n",
      "Epoch 66/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.8364, Loss_G_Adv=8.5547, Loss_G_Clip=4.8599] \n",
      "Epoch 67/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.0491, Loss_G_Adv=3.0586, Loss_G_Clip=4.8775] \n",
      "Epoch 68/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.4482, Loss_G_Adv=3.2070, Loss_G_Clip=4.8640] \n",
      "Epoch 69/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.4474, Loss_G_Adv=5.6484, Loss_G_Clip=4.8640] \n",
      "Epoch 70/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.8294, Loss_G_Adv=-0.4846, Loss_G_Clip=4.8595]\n",
      "Epoch 71/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.2508, Loss_G_Adv=3.9395, Loss_G_Clip=4.8616] \n",
      "Epoch 72/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.1045, Loss_G_Adv=-0.9331, Loss_G_Clip=4.8604]\n",
      "Epoch 73/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.9456, Loss_G_Adv=3.1523, Loss_G_Clip=4.8526] \n",
      "Epoch 74/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.1967, Loss_G_Adv=2.9355, Loss_G_Clip=4.8806] \n",
      "Epoch 75/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.8673, Loss_G_Adv=3.6562, Loss_G_Clip=4.8608] \n",
      "Epoch 76/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.4341, Loss_G_Adv=-0.9321, Loss_G_Clip=4.8642]\n",
      "Epoch 77/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.0070, Loss_G_Adv=-0.5386, Loss_G_Clip=4.8656]\n",
      "Epoch 78/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.0052, Loss_G_Adv=-0.7944, Loss_G_Clip=4.8654]\n",
      "Epoch 79/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.3869, Loss_G_Adv=-1.8477, Loss_G_Clip=4.8668]\n",
      "Epoch 80/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.3810, Loss_G_Adv=-2.4570, Loss_G_Clip=4.8618]\n",
      "Epoch 81/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.2684, Loss_G_Adv=0.2556, Loss_G_Clip=4.8667] \n",
      "Epoch 82/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.0915, Loss_G_Adv=-3.8477, Loss_G_Clip=4.8510]\n",
      "Epoch 83/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.0147, Loss_G_Adv=0.1808, Loss_G_Clip=4.8747] \n",
      "Epoch 84/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.4299, Loss_G_Adv=1.5996, Loss_G_Clip=4.8659] \n",
      "Epoch 85/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.0216, Loss_G_Adv=0.0371, Loss_G_Clip=4.8578] \n",
      "Epoch 86/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.3302, Loss_G_Adv=1.5459, Loss_G_Clip=4.8551] \n",
      "Epoch 87/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7398, Loss_G_Adv=-4.0430, Loss_G_Clip=4.8615]\n",
      "Epoch 88/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.4310, Loss_G_Adv=-0.3125, Loss_G_Clip=4.8670]\n",
      "Epoch 89/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.2537, Loss_G_Adv=-0.4529, Loss_G_Clip=4.8681]\n",
      "Epoch 90/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.9748, Loss_G_Adv=1.1348, Loss_G_Clip=4.8555] \n",
      "Epoch 91/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.1013, Loss_G_Adv=-2.4570, Loss_G_Clip=4.8748]\n",
      "Epoch 92/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.0437, Loss_G_Adv=-1.4990, Loss_G_Clip=4.8633]\n",
      "Epoch 93/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.6688, Loss_G_Adv=-0.8193, Loss_G_Clip=4.8724]\n",
      "Epoch 94/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.1186, Loss_G_Adv=-1.1357, Loss_G_Clip=4.8553]\n",
      "Epoch 95/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.0917, Loss_G_Adv=-4.9141, Loss_G_Clip=4.8584]\n",
      "Epoch 96/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6897, Loss_G_Adv=-4.8281, Loss_G_Clip=4.8602]\n",
      "Epoch 97/600: 100%|██████████| 92/92 [02:23<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.9034, Loss_G_Adv=-0.2812, Loss_G_Clip=4.8634]\n",
      "Epoch 98/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.7928, Loss_G_Adv=-6.5508, Loss_G_Clip=4.8610]\n",
      "Epoch 99/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.0412, Loss_G_Adv=-1.1230, Loss_G_Clip=4.8717]\n",
      "Epoch 100/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.2618, Loss_G_Adv=-7.8633, Loss_G_Clip=4.8627]\n",
      "Epoch 101/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.5622, Loss_G_Adv=-2.5137, Loss_G_Clip=4.8651]\n",
      "Epoch 102/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.9095, Loss_G_Adv=-3.7891, Loss_G_Clip=4.8567]\n",
      "Epoch 103/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.2079, Loss_G_Adv=-4.8164, Loss_G_Clip=4.8456]\n",
      "Epoch 104/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6488, Loss_G_Adv=-7.0938, Loss_G_Clip=4.8603] \n",
      "Epoch 105/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.7606, Loss_G_Adv=-10.4688, Loss_G_Clip=4.8593]\n",
      "Epoch 106/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.1019, Loss_G_Adv=-7.7617, Loss_G_Clip=4.8441] \n",
      "Epoch 107/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.6750, Loss_G_Adv=-11.0625, Loss_G_Clip=4.8559]\n",
      "Epoch 108/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.0591, Loss_G_Adv=-7.9531, Loss_G_Clip=4.8531] \n",
      "Epoch 109/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.9702, Loss_G_Adv=-8.9688, Loss_G_Clip=4.8524] \n",
      "Epoch 110/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.7560, Loss_G_Adv=-12.0859, Loss_G_Clip=4.8618]\n",
      "Epoch 111/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6438, Loss_G_Adv=-7.9609, Loss_G_Clip=4.8596] \n",
      "Epoch 112/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7347, Loss_G_Adv=-11.5391, Loss_G_Clip=4.8603]\n",
      "Epoch 113/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.9026, Loss_G_Adv=-11.0781, Loss_G_Clip=4.8591]\n",
      "Epoch 114/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7873, Loss_G_Adv=-14.4141, Loss_G_Clip=4.8587]\n",
      "Epoch 115/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.9727, Loss_G_Adv=-19.3438, Loss_G_Clip=4.8500]\n",
      "Epoch 116/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.0074, Loss_G_Adv=-11.3047, Loss_G_Clip=4.8627]\n",
      "Epoch 117/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.3829, Loss_G_Adv=-21.1719, Loss_G_Clip=4.8649]\n",
      "Epoch 118/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.8950, Loss_G_Adv=-12.0000, Loss_G_Clip=4.8464]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.5264, Loss_G_Adv=-19.1094, Loss_G_Clip=4.8620]\n",
      "Epoch 120/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6829, Loss_G_Adv=-16.4062, Loss_G_Clip=4.8521]\n",
      "Epoch 121/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.2800, Loss_G_Adv=-14.2656, Loss_G_Clip=4.8566]\n",
      "Epoch 122/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.2175, Loss_G_Adv=-17.5938, Loss_G_Clip=4.8481]\n",
      "Epoch 123/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.0781, Loss_G_Adv=-17.1719, Loss_G_Clip=4.8623]\n",
      "Epoch 124/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.0892, Loss_G_Adv=-21.1250, Loss_G_Clip=4.8511]\n",
      "Epoch 125/600: 100%|██████████| 92/92 [02:23<00:00,  1.56s/it, Lambda_Clip=0.200, Loss_D=-6.6880, Loss_G_Adv=-19.6719, Loss_G_Clip=4.8627]\n",
      "Epoch 126/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.4663, Loss_G_Adv=-19.7344, Loss_G_Clip=4.8449]\n",
      "Epoch 127/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.1215, Loss_G_Adv=-24.1406, Loss_G_Clip=4.8606]\n",
      "Epoch 128/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.4487, Loss_G_Adv=-26.5312, Loss_G_Clip=4.8618]\n",
      "Epoch 129/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.9504, Loss_G_Adv=-21.5469, Loss_G_Clip=4.8507]\n",
      "Epoch 130/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.0371, Loss_G_Adv=-19.8438, Loss_G_Clip=4.8573]\n",
      "Epoch 131/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6690, Loss_G_Adv=-24.5625, Loss_G_Clip=4.8491]\n",
      "Epoch 132/600: 100%|██████████| 92/92 [02:23<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.2623, Loss_G_Adv=-22.8281, Loss_G_Clip=4.8543]\n",
      "Epoch 133/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.1145, Loss_G_Adv=-26.2031, Loss_G_Clip=4.8579]\n",
      "Epoch 134/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6943, Loss_G_Adv=-30.8125, Loss_G_Clip=4.8578]\n",
      "Epoch 135/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.3129, Loss_G_Adv=-28.7500, Loss_G_Clip=4.8586]\n",
      "Epoch 136/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.1154, Loss_G_Adv=-29.3750, Loss_G_Clip=4.8529]\n",
      "Epoch 137/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.7100, Loss_G_Adv=-23.8125, Loss_G_Clip=4.8506]\n",
      "Epoch 138/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.4003, Loss_G_Adv=-28.1250, Loss_G_Clip=4.8531]\n",
      "Epoch 139/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.4477, Loss_G_Adv=-24.6562, Loss_G_Clip=4.8605]\n",
      "Epoch 140/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.1543, Loss_G_Adv=-25.7969, Loss_G_Clip=4.8579]\n",
      "Epoch 141/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.0104, Loss_G_Adv=-34.5938, Loss_G_Clip=4.8575]\n",
      "Epoch 142/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.3648, Loss_G_Adv=-33.5625, Loss_G_Clip=4.8532]\n",
      "Epoch 143/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.9156, Loss_G_Adv=-35.6875, Loss_G_Clip=4.8541]\n",
      "Epoch 144/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.2322, Loss_G_Adv=-30.4062, Loss_G_Clip=4.8494]\n",
      "Epoch 145/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7152, Loss_G_Adv=-37.5625, Loss_G_Clip=4.8582]\n",
      "Epoch 146/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7705, Loss_G_Adv=-37.5938, Loss_G_Clip=4.8554]\n",
      "Epoch 147/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.7812, Loss_G_Adv=-38.7500, Loss_G_Clip=4.8613]\n",
      "Epoch 148/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.7941, Loss_G_Adv=-41.0625, Loss_G_Clip=4.8511]\n",
      "Epoch 149/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.9264, Loss_G_Adv=-42.5000, Loss_G_Clip=4.8554]\n",
      "Epoch 150/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7547, Loss_G_Adv=-44.4375, Loss_G_Clip=4.8526]\n",
      "Epoch 151/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.2672, Loss_G_Adv=-38.6562, Loss_G_Clip=4.8569]\n",
      "Epoch 152/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.9845, Loss_G_Adv=-42.8125, Loss_G_Clip=4.8428]\n",
      "Epoch 153/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.2736, Loss_G_Adv=-38.8438, Loss_G_Clip=4.8529]\n",
      "Epoch 154/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7492, Loss_G_Adv=-44.2188, Loss_G_Clip=4.8524]\n",
      "Epoch 155/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.6017, Loss_G_Adv=-45.8125, Loss_G_Clip=4.8553]\n",
      "Epoch 156/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.1235, Loss_G_Adv=-41.9688, Loss_G_Clip=4.8456]\n",
      "Epoch 157/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.5117, Loss_G_Adv=-48.3750, Loss_G_Clip=4.8518]\n",
      "Epoch 158/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.1373, Loss_G_Adv=-48.4375, Loss_G_Clip=4.8593]\n",
      "Epoch 159/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7023, Loss_G_Adv=-49.1562, Loss_G_Clip=4.8515]\n",
      "Epoch 160/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.5846, Loss_G_Adv=-50.8125, Loss_G_Clip=4.8543]\n",
      "Epoch 161/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.2754, Loss_G_Adv=-47.8750, Loss_G_Clip=4.8606]\n",
      "Epoch 162/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.6722, Loss_G_Adv=-54.8750, Loss_G_Clip=4.8512]\n",
      "Epoch 163/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6392, Loss_G_Adv=-57.5312, Loss_G_Clip=4.8490]\n",
      "Epoch 164/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.4863, Loss_G_Adv=-57.7812, Loss_G_Clip=4.8557]\n",
      "Epoch 165/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.5976, Loss_G_Adv=-52.7500, Loss_G_Clip=4.8565]\n",
      "Epoch 166/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.9784, Loss_G_Adv=-56.9375, Loss_G_Clip=4.8531]\n",
      "Epoch 167/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6665, Loss_G_Adv=-53.9688, Loss_G_Clip=4.8534]\n",
      "Epoch 168/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.3015, Loss_G_Adv=-61.8438, Loss_G_Clip=4.8552]\n",
      "Epoch 169/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.6597, Loss_G_Adv=-60.7812, Loss_G_Clip=4.8518]\n",
      "Epoch 170/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.6271, Loss_G_Adv=-62.5312, Loss_G_Clip=4.8525]\n",
      "Epoch 171/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.7223, Loss_G_Adv=-62.3125, Loss_G_Clip=4.8544]\n",
      "Epoch 172/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.2820, Loss_G_Adv=-58.9688, Loss_G_Clip=4.8596]\n",
      "Epoch 173/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6140, Loss_G_Adv=-67.8750, Loss_G_Clip=4.8551]\n",
      "Epoch 174/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-3.9838, Loss_G_Adv=-69.8125, Loss_G_Clip=4.8491]\n",
      "Epoch 175/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.8232, Loss_G_Adv=-68.2500, Loss_G_Clip=4.8539]\n",
      "Epoch 176/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6449, Loss_G_Adv=-64.7500, Loss_G_Clip=4.8562]\n",
      "Epoch 177/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.8859, Loss_G_Adv=-71.4375, Loss_G_Clip=4.8548]\n",
      "Epoch 178/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.3529, Loss_G_Adv=-73.6875, Loss_G_Clip=4.8545]\n",
      "Epoch 179/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.3996, Loss_G_Adv=-70.4375, Loss_G_Clip=4.8603]\n",
      "Epoch 180/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.9575, Loss_G_Adv=-71.1250, Loss_G_Clip=4.8533]\n",
      "Epoch 181/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.2874, Loss_G_Adv=-77.8125, Loss_G_Clip=4.8533]\n",
      "Epoch 182/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.1817, Loss_G_Adv=-74.2500, Loss_G_Clip=4.8468]\n",
      "Epoch 183/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.7671, Loss_G_Adv=-77.6875, Loss_G_Clip=4.8518]\n",
      "Epoch 184/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7723, Loss_G_Adv=-79.3125, Loss_G_Clip=4.8485]\n",
      "Epoch 185/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.1694, Loss_G_Adv=-78.8125, Loss_G_Clip=4.8548]\n",
      "Epoch 186/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6849, Loss_G_Adv=-81.6250, Loss_G_Clip=4.8557]\n",
      "Epoch 187/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.5357, Loss_G_Adv=-87.5625, Loss_G_Clip=4.8583]\n",
      "Epoch 188/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.2449, Loss_G_Adv=-79.5000, Loss_G_Clip=4.8499]\n",
      "Epoch 189/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.8930, Loss_G_Adv=-82.6875, Loss_G_Clip=4.8485]\n",
      "Epoch 190/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.5773, Loss_G_Adv=-86.0000, Loss_G_Clip=4.8614]\n",
      "Epoch 191/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.4926, Loss_G_Adv=-90.1250, Loss_G_Clip=4.8531]\n",
      "Epoch 192/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.4374, Loss_G_Adv=-84.9375, Loss_G_Clip=4.8498]\n",
      "Epoch 193/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.0370, Loss_G_Adv=-86.2500, Loss_G_Clip=4.8521]\n",
      "Epoch 194/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6219, Loss_G_Adv=-91.2500, Loss_G_Clip=4.8489]\n",
      "Epoch 195/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.6858, Loss_G_Adv=-86.0000, Loss_G_Clip=4.8611]\n",
      "Epoch 196/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6776, Loss_G_Adv=-91.7500, Loss_G_Clip=4.8485]\n",
      "Epoch 197/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.3084, Loss_G_Adv=-92.5000, Loss_G_Clip=4.8589]\n",
      "Epoch 198/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.3633, Loss_G_Adv=-94.8750, Loss_G_Clip=4.8549]\n",
      "Epoch 199/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7655, Loss_G_Adv=-92.5000, Loss_G_Clip=4.8531] \n",
      "Epoch 200/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.4333, Loss_G_Adv=-97.4375, Loss_G_Clip=4.8550] \n",
      "Epoch 201/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6429, Loss_G_Adv=-95.5625, Loss_G_Clip=4.8531] \n",
      "Epoch 202/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.2256, Loss_G_Adv=-93.5625, Loss_G_Clip=4.8505] \n",
      "Epoch 203/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.4845, Loss_G_Adv=-92.8750, Loss_G_Clip=4.8519] \n",
      "Epoch 204/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.4718, Loss_G_Adv=-104.4375, Loss_G_Clip=4.8543]\n",
      "Epoch 205/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.8422, Loss_G_Adv=-106.6250, Loss_G_Clip=4.8519]\n",
      "Epoch 206/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.5340, Loss_G_Adv=-100.9375, Loss_G_Clip=4.8487]\n",
      "Epoch 207/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.7409, Loss_G_Adv=-100.7500, Loss_G_Clip=4.8538]\n",
      "Epoch 208/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.1814, Loss_G_Adv=-104.0000, Loss_G_Clip=4.8558]\n",
      "Epoch 209/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.6943, Loss_G_Adv=-106.7500, Loss_G_Clip=4.8502]\n",
      "Epoch 210/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.5957, Loss_G_Adv=-110.0625, Loss_G_Clip=4.8572]\n",
      "Epoch 211/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.3069, Loss_G_Adv=-104.6875, Loss_G_Clip=4.8575]\n",
      "Epoch 212/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.2472, Loss_G_Adv=-111.8750, Loss_G_Clip=4.8499]\n",
      "Epoch 213/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.9256, Loss_G_Adv=-112.8125, Loss_G_Clip=4.8476]\n",
      "Epoch 214/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.3424, Loss_G_Adv=-115.1250, Loss_G_Clip=4.8547]\n",
      "Epoch 215/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.3430, Loss_G_Adv=-113.0625, Loss_G_Clip=4.8474]\n",
      "Epoch 216/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.2410, Loss_G_Adv=-118.9375, Loss_G_Clip=4.8492]\n",
      "Epoch 217/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.1998, Loss_G_Adv=-116.2500, Loss_G_Clip=4.8560]\n",
      "Epoch 218/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.1150, Loss_G_Adv=-116.7500, Loss_G_Clip=4.8525]\n",
      "Epoch 219/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.5282, Loss_G_Adv=-120.6875, Loss_G_Clip=4.8528]\n",
      "Epoch 220/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.6319, Loss_G_Adv=-121.3750, Loss_G_Clip=4.8536]\n",
      "Epoch 221/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.2188, Loss_G_Adv=-122.4375, Loss_G_Clip=4.8526]\n",
      "Epoch 222/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.4335, Loss_G_Adv=-126.0625, Loss_G_Clip=4.8531]\n",
      "Epoch 223/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.5895, Loss_G_Adv=-131.2500, Loss_G_Clip=4.8560]\n",
      "Epoch 224/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.1058, Loss_G_Adv=-126.6250, Loss_G_Clip=4.8558]\n",
      "Epoch 225/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.3639, Loss_G_Adv=-126.8125, Loss_G_Clip=4.8560]\n",
      "Epoch 226/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.0901, Loss_G_Adv=-132.8750, Loss_G_Clip=4.8519]\n",
      "Epoch 227/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-4.3608, Loss_G_Adv=-120.5000, Loss_G_Clip=4.8505]\n",
      "Epoch 228/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6571, Loss_G_Adv=-127.1875, Loss_G_Clip=4.8518]\n",
      "Epoch 229/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.6829, Loss_G_Adv=-131.0000, Loss_G_Clip=4.8512]\n",
      "Epoch 230/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.3267, Loss_G_Adv=-132.7500, Loss_G_Clip=4.8510]\n",
      "Epoch 231/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.0588, Loss_G_Adv=-132.0000, Loss_G_Clip=4.8515]\n",
      "Epoch 232/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.0898, Loss_G_Adv=-137.8750, Loss_G_Clip=4.8504]\n",
      "Epoch 233/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7841, Loss_G_Adv=-134.8750, Loss_G_Clip=4.8519]\n",
      "Epoch 234/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.9962, Loss_G_Adv=-137.7500, Loss_G_Clip=4.8548]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 235/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-8.7035, Loss_G_Adv=-144.0000, Loss_G_Clip=4.8583]\n",
      "Epoch 236/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.7102, Loss_G_Adv=-135.6250, Loss_G_Clip=4.8547]\n",
      "Epoch 237/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.9208, Loss_G_Adv=-143.0000, Loss_G_Clip=4.8524]\n",
      "Epoch 238/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.1722, Loss_G_Adv=-147.7500, Loss_G_Clip=4.8578]\n",
      "Epoch 239/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.3123, Loss_G_Adv=-139.1250, Loss_G_Clip=4.8511]\n",
      "Epoch 240/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.7049, Loss_G_Adv=-148.6250, Loss_G_Clip=4.8507]\n",
      "Epoch 241/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.9884, Loss_G_Adv=-148.1250, Loss_G_Clip=4.8557]\n",
      "Epoch 242/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6931, Loss_G_Adv=-140.6250, Loss_G_Clip=4.8511]\n",
      "Epoch 243/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.8928, Loss_G_Adv=-147.3750, Loss_G_Clip=4.8532]\n",
      "Epoch 244/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.4629, Loss_G_Adv=-153.5000, Loss_G_Clip=4.8515]\n",
      "Epoch 245/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.7935, Loss_G_Adv=-150.1250, Loss_G_Clip=4.8495]\n",
      "Epoch 246/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.2972, Loss_G_Adv=-156.0000, Loss_G_Clip=4.8554]\n",
      "Epoch 247/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.1881, Loss_G_Adv=-151.3750, Loss_G_Clip=4.8472]\n",
      "Epoch 248/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.5528, Loss_G_Adv=-148.0000, Loss_G_Clip=4.8546]\n",
      "Epoch 249/600: 100%|██████████| 92/92 [02:23<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.9611, Loss_G_Adv=-158.8750, Loss_G_Clip=4.8519]\n",
      "Epoch 250/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.3885, Loss_G_Adv=-159.8750, Loss_G_Clip=4.8574]\n",
      "Epoch 251/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.3787, Loss_G_Adv=-154.1250, Loss_G_Clip=4.8557]\n",
      "Epoch 252/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.0059, Loss_G_Adv=-165.6250, Loss_G_Clip=4.8534]\n",
      "Epoch 253/600: 100%|██████████| 92/92 [02:23<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.5723, Loss_G_Adv=-166.1250, Loss_G_Clip=4.8522]\n",
      "Epoch 254/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.2003, Loss_G_Adv=-161.1250, Loss_G_Clip=4.8487]\n",
      "Epoch 255/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.6515, Loss_G_Adv=-165.8750, Loss_G_Clip=4.8525]\n",
      "Epoch 256/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.3369, Loss_G_Adv=-157.8750, Loss_G_Clip=4.8504]\n",
      "Epoch 257/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.9565, Loss_G_Adv=-173.6250, Loss_G_Clip=4.8555]\n",
      "Epoch 258/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.1293, Loss_G_Adv=-165.5000, Loss_G_Clip=4.8516]\n",
      "Epoch 259/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.6577, Loss_G_Adv=-164.5000, Loss_G_Clip=4.8547]\n",
      "Epoch 260/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.2747, Loss_G_Adv=-172.1250, Loss_G_Clip=4.8495]\n",
      "Epoch 261/600: 100%|██████████| 92/92 [02:23<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.1012, Loss_G_Adv=-168.2500, Loss_G_Clip=4.8520]\n",
      "Epoch 262/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.5015, Loss_G_Adv=-174.8750, Loss_G_Clip=4.8544]\n",
      "Epoch 263/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.6480, Loss_G_Adv=-172.6250, Loss_G_Clip=4.8514]\n",
      "Epoch 264/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.1096, Loss_G_Adv=-170.0000, Loss_G_Clip=4.8516]\n",
      "Epoch 265/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.5749, Loss_G_Adv=-179.8750, Loss_G_Clip=4.8479]\n",
      "Epoch 266/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.4525, Loss_G_Adv=-177.7500, Loss_G_Clip=4.8517]\n",
      "Epoch 267/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.8131, Loss_G_Adv=-180.0000, Loss_G_Clip=4.8488]\n",
      "Epoch 268/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.7842, Loss_G_Adv=-182.6250, Loss_G_Clip=4.8515]\n",
      "Epoch 269/600: 100%|██████████| 92/92 [02:23<00:00,  1.56s/it, Lambda_Clip=0.200, Loss_D=-5.8961, Loss_G_Adv=-181.1250, Loss_G_Clip=4.8543]\n",
      "Epoch 270/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.4781, Loss_G_Adv=-188.3750, Loss_G_Clip=4.8521]\n",
      "Epoch 271/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.8497, Loss_G_Adv=-187.8750, Loss_G_Clip=4.8552]\n",
      "Epoch 272/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.4908, Loss_G_Adv=-187.0000, Loss_G_Clip=4.8481]\n",
      "Epoch 273/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.2826, Loss_G_Adv=-190.7500, Loss_G_Clip=4.8499]\n",
      "Epoch 274/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.2073, Loss_G_Adv=-185.8750, Loss_G_Clip=4.8500]\n",
      "Epoch 275/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-8.8110, Loss_G_Adv=-191.7500, Loss_G_Clip=4.8532]\n",
      "Epoch 276/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.5699, Loss_G_Adv=-193.7500, Loss_G_Clip=4.8508]\n",
      "Epoch 277/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.2907, Loss_G_Adv=-193.3750, Loss_G_Clip=4.8523]\n",
      "Epoch 278/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.9729, Loss_G_Adv=-200.1250, Loss_G_Clip=4.8538]\n",
      "Epoch 279/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.9630, Loss_G_Adv=-200.3750, Loss_G_Clip=4.8492]\n",
      "Epoch 280/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-5.9989, Loss_G_Adv=-193.7500, Loss_G_Clip=4.8518]\n",
      "Epoch 281/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.9599, Loss_G_Adv=-201.0000, Loss_G_Clip=4.8481]\n",
      "Epoch 282/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-9.3142, Loss_G_Adv=-207.5000, Loss_G_Clip=4.8510]\n",
      "Epoch 283/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.9525, Loss_G_Adv=-201.5000, Loss_G_Clip=4.8520]\n",
      "Epoch 284/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.1124, Loss_G_Adv=-203.1250, Loss_G_Clip=4.8488] \n",
      "Epoch 285/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.7233, Loss_G_Adv=-212.1250, Loss_G_Clip=4.8567] \n",
      "Epoch 286/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-8.3501, Loss_G_Adv=-210.6250, Loss_G_Clip=4.8486] \n",
      "Epoch 287/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.7430, Loss_G_Adv=-213.1250, Loss_G_Clip=4.8517] \n",
      "Epoch 288/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-8.8886, Loss_G_Adv=-214.6250, Loss_G_Clip=4.8546]\n",
      "Epoch 289/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.2493, Loss_G_Adv=-207.7500, Loss_G_Clip=4.8537]\n",
      "Epoch 290/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.4867, Loss_G_Adv=-219.6250, Loss_G_Clip=4.8517] \n",
      "Epoch 291/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-8.0529, Loss_G_Adv=-219.2500, Loss_G_Clip=4.8526] \n",
      "Epoch 292/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.6525, Loss_G_Adv=-217.0000, Loss_G_Clip=4.8502] \n",
      "Epoch 293/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-10.1941, Loss_G_Adv=-227.6250, Loss_G_Clip=4.8528]\n",
      "Epoch 294/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-7.4836, Loss_G_Adv=-219.3750, Loss_G_Clip=4.8540] \n",
      "Epoch 295/600: 100%|██████████| 92/92 [02:23<00:00,  1.56s/it, Lambda_Clip=0.200, Loss_D=-8.0028, Loss_G_Adv=-220.7500, Loss_G_Clip=4.8505] \n",
      "Epoch 296/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-9.2617, Loss_G_Adv=-235.6250, Loss_G_Clip=4.8514] \n",
      "Epoch 297/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.6893, Loss_G_Adv=-230.1250, Loss_G_Clip=4.8504] \n",
      "Epoch 298/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-8.4518, Loss_G_Adv=-232.3750, Loss_G_Clip=4.8512] \n",
      "Epoch 299/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-8.3122, Loss_G_Adv=-234.0000, Loss_G_Clip=4.8512] \n",
      "Epoch 300/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-6.7156, Loss_G_Adv=-229.8750, Loss_G_Clip=4.8528] \n",
      "Epoch 301/600: 100%|██████████| 92/92 [02:22<00:00,  1.55s/it, Lambda_Clip=0.200, Loss_D=-9.8348, Loss_G_Adv=-238.5000, Loss_G_Clip=4.8559] \n",
      "Epoch 302/600:  55%|█████▌    | 51/92 [01:21<01:03,  1.54s/it, Lambda_Clip=0.200, Loss_D=-9.3486, Loss_G_Adv=-237.6250, Loss_G_Clip=4.8531] "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torchvision.utils as vutils\n",
    "import clip\n",
    "import random\n",
    "from torch.nn.utils import spectral_norm, clip_grad_norm_\n",
    "from torch.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration class for all hyperparameters and settings.\n",
    "    Adjusted for a 16GB GPU and ~10 hour training session.\n",
    "    \"\"\"\n",
    "    # --- Paths ---\n",
    "    IMG_ROOT = \"/kaggle/working/images/CUB_200_2011/images\"\n",
    "    TEXT_ROOT = \"/kaggle/working/raw_texts/cvpr2016_cub/text_c10\"\n",
    "    \n",
    "    # --- Model & Hardware ---\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    CLIP_MODEL = \"ViT-B/32\"\n",
    "    \n",
    "    # --- Training Hyperparameters ---\n",
    "    # BATCH_SIZE lowered to fit 128x128 model in ~16GB VRAM\n",
    "    BATCH_SIZE = 128 \n",
    "    # EPOCHS increased for longer training at higher resolution\n",
    "    EPOCHS = 600\n",
    "    # TTUR (Two-Time-Scale Update Rule) learning rates\n",
    "    LR_G = 1e-4 \n",
    "    LR_D = 4e-4\n",
    "    BETA1 = 0.0\n",
    "    BETA2 = 0.9\n",
    "    \n",
    "    # --- Architecture ---\n",
    "    IMG_SIZE = 128 # Increased image resolution\n",
    "    Z_DIM = 100\n",
    "    IMG_CHANNELS = 3\n",
    "    G_FEATURES = 128\n",
    "    D_FEATURES = 64\n",
    "    \n",
    "    # --- Loss Weights & Regularization ---\n",
    "    LAMBDA_GP = 10\n",
    "    # Contrastive loss weight lowered to balance with adversarial loss\n",
    "    LAMBDA_CONTRASTIVE = 0.2 \n",
    "    CONTRASTIVE_TEMP = 0.07\n",
    "    CONTRASTIVE_WARMUP_EPOCHS = 20 # Longer warmup for stability\n",
    "    GRAD_CLIP = 5.0\n",
    "    EMA_DECAY = 0.999 # Decay for Generator's Exponential Moving Average\n",
    "\n",
    "    # --- Training Schedule & Saving ---\n",
    "    D_STEPS_PER_G_STEP = 1\n",
    "    SAVE_FREQ = 20 # Save checkpoints less frequently to save time\n",
    "    \n",
    "\n",
    "class Text2ImageDataset(Dataset):\n",
    "    def __init__(self, img_root, text_root, transform=None, tokenizer=None):\n",
    "        self.tokenizer = tokenizer or (lambda t: clip.tokenize(t, truncate=True))\n",
    "        \n",
    "        # Added RandomHorizontalFlip for data augmentation\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(Config.IMG_SIZE),\n",
    "            transforms.RandomCrop(Config.IMG_SIZE),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        self.pairs = []\n",
    "        # Fallback for environments where the data isn't present\n",
    "        if not os.path.exists(img_root) or not os.path.exists(text_root):\n",
    "              print(f\"Warning: Data directory not found. Using placeholder data.\")\n",
    "              exit()\n",
    "        else:\n",
    "            for root, _, files in os.walk(img_root):\n",
    "                for file in files:\n",
    "                    if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                        img_path = os.path.join(root, file)\n",
    "                        txt_filename = os.path.splitext(file)[0] + \".txt\"\n",
    "                        rel_path_dir = os.path.relpath(root, img_root)\n",
    "                        text_path = os.path.join(text_root, rel_path_dir, txt_filename)\n",
    "                        \n",
    "                        if os.path.exists(text_path):\n",
    "                            self.pairs.append((img_path, text_path))\n",
    "\n",
    "        print(f\"Loaded {len(self.pairs)} image-text pairs.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, text_info = self.pairs[idx]\n",
    "        try:\n",
    "            if os.path.exists(img_path):\n",
    "                image = read_image(img_path)\n",
    "                with open(text_info, 'r', encoding='utf-8') as f:\n",
    "                    captions = [line.strip() for line in f if line.strip()]\n",
    "                caption = random.choice(captions)\n",
    "            else: # Fallback for dummy data\n",
    "                exit()\n",
    "\n",
    "            # Handle grayscale images\n",
    "            if image.shape[0] == 1:\n",
    "                image = image.repeat(3, 1, 1)\n",
    "            \n",
    "            image = self.transform(image)\n",
    "            \n",
    "            return image, caption\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Skipping corrupted item: {img_path} | Reason: {e}\")\n",
    "            # Return a random different item on error\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "class CLIPImageEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.model = clip_model.visual\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                                 std=(0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = (x + 1) / 2 # from [-1, 1] to [0, 1]\n",
    "        x = self.preprocess(x)\n",
    "        return self.model(x.float())\n",
    "\n",
    "class CLIPTextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.model = clip_model\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, text):\n",
    "        tokens = clip.tokenize(text, truncate=True).to(Config.DEVICE)\n",
    "        embed = self.model.encode_text(tokens)\n",
    "        return embed / embed.norm(dim=-1, keepdim=True)\n",
    "\n",
    "class ConditionalNorm(nn.Module):\n",
    "    def __init__(self, feat_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.norm = nn.BatchNorm2d(feat_size, affine=False)\n",
    "        self.gamma = spectral_norm(nn.Linear(embed_size, feat_size))\n",
    "        self.beta = spectral_norm(nn.Linear(embed_size, feat_size))\n",
    "        \n",
    "    def forward(self, x, emb):\n",
    "        x = self.norm(x)\n",
    "        gamma = self.gamma(emb)[:,:,None,None]\n",
    "        beta = self.beta(emb)[:,:,None,None]\n",
    "        return x * (1 + gamma) + beta\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv = spectral_norm(nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False))\n",
    "        self.cond_norm = ConditionalNorm(out_channels, embed_dim)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        \n",
    "    def forward(self, x, emb):\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.cond_norm(x, emb)\n",
    "        return self.relu(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, embed_dim=512, projected_embed_dim=128):\n",
    "        super().__init__()\n",
    "        ngf = Config.G_FEATURES\n",
    "        self.text_projection = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(embed_dim, projected_embed_dim)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.noise_projection = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(Config.Z_DIM, 4 * 4 * ngf * 8)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Added an extra UpBlock for 128x128 resolution\n",
    "        self.main = nn.ModuleList([\n",
    "            UpBlock(ngf * 8, ngf * 8, projected_embed_dim), # 4x4 -> 8x8\n",
    "            UpBlock(ngf * 8, ngf * 4, projected_embed_dim), # 8x8 -> 16x16\n",
    "            UpBlock(ngf * 4, ngf * 2, projected_embed_dim), # 16x16 -> 32x32\n",
    "            UpBlock(ngf * 2, ngf, projected_embed_dim),     # 32x32 -> 64x64\n",
    "            UpBlock(ngf, ngf // 2, projected_embed_dim),    # 64x64 -> 128x128\n",
    "        ])\n",
    "        self.to_rgb = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(ngf // 2, Config.IMG_CHANNELS, 3, 1, 1, bias=False)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, embed_vector, z):\n",
    "        cond_vector = self.text_projection(embed_vector)\n",
    "        noise_proj = self.noise_projection(z).view(-1, Config.G_FEATURES * 8, 4, 4)\n",
    "        x = noise_proj\n",
    "        for layer in self.main:\n",
    "            x = layer(x, cond_vector)\n",
    "        return self.to_rgb(x)\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, norm=True):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            spectral_norm(nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)),\n",
    "        ]\n",
    "        if norm:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_dim=512, projected_embed_dim=128):\n",
    "        super().__init__()\n",
    "        ndf = Config.D_FEATURES\n",
    "        # DownBlocks for 128x128 resolution\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            DownBlock(Config.IMG_CHANNELS, ndf, norm=False), # 128->64\n",
    "            DownBlock(ndf, ndf * 2),                         # 64->32\n",
    "            DownBlock(ndf * 2, ndf * 4),                     # 32->16\n",
    "            DownBlock(ndf * 4, ndf * 8),                     # 16->8\n",
    "            DownBlock(ndf * 8, ndf * 8),                     # 8->4\n",
    "        )\n",
    "        self.text_projection = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(embed_dim, projected_embed_dim)),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.joint_conv = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(ndf * 8 + projected_embed_dim, ndf * 8, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.final_conv = nn.Conv2d(ndf * 8, 1, 4, 1, 0)\n",
    "\n",
    "    def forward(self, img, embed):\n",
    "        img_feat = self.conv_blocks(img)\n",
    "        txt_feat = self.text_projection(embed)\n",
    "        txt_feat = txt_feat.view(txt_feat.size(0), -1, 1, 1).expand(\n",
    "            -1, -1, img_feat.size(2), img_feat.size(3)\n",
    "        )\n",
    "        combined = torch.cat([img_feat, txt_feat], dim=1)\n",
    "        combined = self.joint_conv(combined)\n",
    "        return self.final_conv(combined).view(combined.size(0), -1)\n",
    "\n",
    "def gradient_penalty(critic, real_images, fake_images, text_embeddings, device):\n",
    "    bs = real_images.size(0)\n",
    "    epsilon = torch.rand(bs, 1, 1, 1, device=device).expand_as(real_images)\n",
    "    interpolated = epsilon * real_images + (1 - epsilon) * fake_images\n",
    "    interpolated.requires_grad_(True)\n",
    "    mixed_scores = critic(interpolated, text_embeddings)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=mixed_scores,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(bs, -1)\n",
    "    grad_norm = gradients.norm(2, dim=1)\n",
    "    gp = ((grad_norm - 1) ** 2).mean()\n",
    "    return gp\n",
    "\n",
    "class CLIPContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / temperature))\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, image_features, text_features):\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        labels = torch.arange(len(logits_per_image), device=logits_per_image.device)\n",
    "        loss_i = self.loss_fn(logits_per_image, labels)\n",
    "        loss_t = self.loss_fn(logits_per_image.t(), labels)\n",
    "        return (loss_i + loss_t) / 2\n",
    "\n",
    "# --- Initialization ---\n",
    "print(\"Initializing models and optimizers...\")\n",
    "clip_model, _ = clip.load(Config.CLIP_MODEL, device=Config.DEVICE)\n",
    "clip_model.eval()\n",
    "\n",
    "clip_text_encoder = CLIPTextEncoder(clip_model).to(Config.DEVICE)\n",
    "clip_image_encoder = CLIPImageEncoder(clip_model).to(Config.DEVICE)\n",
    "\n",
    "G = Generator().to(Config.DEVICE)\n",
    "D = Discriminator().to(Config.DEVICE)\n",
    "\n",
    "# Initialize EMA model for generator\n",
    "# The 'decay' argument is not supported in some PyTorch versions.\n",
    "# We define the averaging function manually using avg_fn.\n",
    "ema_avg_fn = lambda averaged_model_parameter, model_parameter, num_averaged: \\\n",
    "    Config.EMA_DECAY * averaged_model_parameter + (1 - Config.EMA_DECAY) * model_parameter\n",
    "ema_G = AveragedModel(G, avg_fn=ema_avg_fn)\n",
    "\n",
    "optimizerD = optim.Adam(D.parameters(), lr=Config.LR_D, betas=(Config.BETA1, Config.BETA2))\n",
    "optimizerG = optim.Adam(G.parameters(), lr=Config.LR_G, betas=(Config.BETA1, Config.BETA2))\n",
    "contrastive_loss_fn = CLIPContrastiveLoss(temperature=Config.CONTRASTIVE_TEMP).to(Config.DEVICE)\n",
    "optimizerCL = optim.Adam(contrastive_loss_fn.parameters(), lr=1e-4) # Slower LR for temperature\n",
    "\n",
    "dataset = Text2ImageDataset(img_root=Config.IMG_ROOT, text_root=Config.TEXT_ROOT)\n",
    "# Use a reasonable number of workers\n",
    "num_workers = min(os.cpu_count(), Config.BATCH_SIZE, 8) if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ else os.cpu_count() // 2\n",
    "dataloader = DataLoader(dataset, batch_size=Config.BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "fixed_noise = torch.randn(64, Config.Z_DIM, device=Config.DEVICE)\n",
    "fixed_captions = [\"a photo of a red bird with a short beak\"] * 8 + \\\n",
    "                 [\"a blue bird with a long thin beak\"] * 8 + \\\n",
    "                 [\"this is a small, brown and white bird with a short, pointed beak\"] * 8 + \\\n",
    "                 [\"a large black bird with a bright yellow crest on its head\"] * 8 + \\\n",
    "                 [\"a photograph of a green and yellow parrot sitting on a branch\"] * 8 + \\\n",
    "                 [\"a beautiful bird with a vibrant plumage of blue and orange\"] * 8 + \\\n",
    "                 [\"a water bird with long slender legs and a curved beak\"] * 8 + \\\n",
    "                 [\"a small yellow finch with black stripes on its wings\"] * 8\n",
    "\n",
    "with torch.no_grad():\n",
    "    fixed_text_embed = clip_text_encoder(fixed_captions).float()\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(Config.EPOCHS):\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{Config.EPOCHS}\")\n",
    "\n",
    "    # Warmup for contrastive loss lambda\n",
    "    if epoch < Config.CONTRASTIVE_WARMUP_EPOCHS:\n",
    "        lambda_contrastive = (epoch / Config.CONTRASTIVE_WARMUP_EPOCHS) * Config.LAMBDA_CONTRASTIVE\n",
    "    else:\n",
    "        lambda_contrastive = Config.LAMBDA_CONTRASTIVE\n",
    "\n",
    "    for batch_idx, (real_images, captions) in enumerate(progress_bar):\n",
    "        real_images = real_images.to(Config.DEVICE)\n",
    "        b_size = real_images.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = clip_text_encoder(captions).float()\n",
    "\n",
    "        # --- Train Discriminator ---\n",
    "        D.train()\n",
    "        optimizerD.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(b_size, Config.Z_DIM, device=Config.DEVICE)\n",
    "            fake_images = G(text_embeddings, noise).detach()\n",
    "\n",
    "        # Run D forward passes with autocast for memory savings\n",
    "        with autocast(device_type=str(Config.DEVICE), dtype=torch.float16):\n",
    "            real_output = D(real_images, text_embeddings)\n",
    "            fake_output = D(fake_images, text_embeddings)\n",
    "            loss_d_real = -torch.mean(real_output)\n",
    "            loss_d_fake = torch.mean(fake_output)\n",
    "            \n",
    "        # Calculate GP in FP32 for stability\n",
    "        gp = gradient_penalty(D, real_images.float(), fake_images.float(), text_embeddings, Config.DEVICE)\n",
    "        \n",
    "        errD = loss_d_real + loss_d_fake + Config.LAMBDA_GP * gp\n",
    "        \n",
    "        scaler.scale(errD).backward()\n",
    "        scaler.step(optimizerD)\n",
    "        scaler.update()\n",
    "\n",
    "        # --- Train Generator + Contrastive loss ---\n",
    "        G.train()\n",
    "        optimizerG.zero_grad()\n",
    "        optimizerCL.zero_grad()\n",
    "\n",
    "        with autocast(device_type=str(Config.DEVICE), dtype=torch.float16):\n",
    "            noise = torch.randn(b_size, Config.Z_DIM, device=Config.DEVICE)\n",
    "            fake_images_for_g = G(text_embeddings, noise)\n",
    "            adv_output = D(fake_images_for_g, text_embeddings)\n",
    "            adv_loss = -torch.mean(adv_output)\n",
    "\n",
    "            # CLIP contrastive loss\n",
    "            fake_image_embeds = clip_image_encoder(fake_images_for_g)\n",
    "            contrastive_loss = contrastive_loss_fn(fake_image_embeds, text_embeddings)\n",
    "            \n",
    "            total_g_loss = adv_loss + lambda_contrastive * contrastive_loss\n",
    "\n",
    "        scaler.scale(total_g_loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizerG)\n",
    "        clip_grad_norm_(G.parameters(), Config.GRAD_CLIP)\n",
    "        \n",
    "        scaler.step(optimizerG)\n",
    "        scaler.step(optimizerCL) # Update temperature param of contrastive loss\n",
    "        scaler.update()\n",
    "\n",
    "        # Update the EMA model - corrected method name\n",
    "        ema_G.update_parameters(G)\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            Loss_D=f\"{errD.item():.4f}\",\n",
    "            Loss_G_Adv=f\"{adv_loss.item():.4f}\",\n",
    "            Loss_G_Clip=f\"{contrastive_loss.item():.4f}\",\n",
    "            Lambda_Clip=f\"{lambda_contrastive:.3f}\"\n",
    "        )\n",
    "\n",
    "    # --- Save samples and models ---\n",
    "    if (epoch + 1) % Config.SAVE_FREQ == 0 or epoch == Config.EPOCHS - 1:\n",
    "        # Use the EMA model for evaluation/saving samples\n",
    "        ema_G.eval()\n",
    "        with torch.no_grad():\n",
    "            # The AveragedModel forward pass is automatically handled\n",
    "            with autocast(device_type=str(Config.DEVICE), dtype=torch.float16):\n",
    "                  fixed_fake = ema_G(fixed_text_embed, fixed_noise).detach().cpu()\n",
    "            vutils.save_image(fixed_fake, f\"sample_ema_epoch_{epoch+1}.png\", normalize=True, nrow=8)\n",
    "        \n",
    "        # Save models\n",
    "        torch.save(G.state_dict(), f\"G_epoch_{epoch+1}.pth\")\n",
    "        torch.save(D.state_dict(), f\"D_epoch_{epoch+1}.pth\")\n",
    "        torch.save(ema_G.state_dict(), f\"ema_G_epoch_{epoch+1}.pth\")\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "torch.save(G.state_dict(), \"G_final.pth\")\n",
    "torch.save(D.state_dict(), \"D_final.pth\")\n",
    "torch.save(ema_G.state_dict(), \"ema_G_final.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-21T18:43:54.618833",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
